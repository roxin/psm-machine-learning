# De l’Intelligence artificielle vers l’apprentissage profond

Avec les machines on peut imiter/simuler une partie des mécanismes conscients du processus de pensée qui sont traduits en algorithmes. On peut qualifier d’intelligent ce type de processus seulement avant de le convertir dans un algorithme, car après, il s’agit plus d’un travail monotone, répétitif, stupide. Le terme d’ **intelligence artificielle** (IA), qui date de 1955 (John McCarthy), ressemble plus à une appellation marketing qu’à une définition scientifique. En effet, l’intelligence ne peut pas être artificielle. « _*Pas plus que la logique, ou le vol. Le vol des avions imite celui des oiseaux, mais on ne parle pas de vol artificiel_* » (Yann LeCun, p. 153).

Conçues de manière à simuler l’intelligence humaine, toutes les formes de l’IA ne sont rien de plus que cela : de la simulation (le « jeu de l’imitation » comme disait Alan Turing - « _Computing machinery and intelligence_ », Mind, 1950). Afin de créer une IA, Turing propose de commencer avec un noyau de base (« _the child program_ ») et le faire évoluer, le former à devenir adulte faisant appel aux mutations, à l’aléatoire et aux essais-erreurs (« _the education process_ » sur la base des processus de récompenses ou de punitions). C’est justement l'apprentissage automatique (_**machine learning**_) qui assure l’évolution du noyau, l'amélioration de ses performances par expérience.
<![endif]-->

L’apprentissage en profondeur (_**deep learning**_) est une technologie qui facilite ce processus.

Ainsi, pour imiter un comportement intelligent, un système d’IA doit « acquérir des connaissances » informelles. Mais, au lieu d’utiliser une description formelle pour reconnaître par exemple un visage dans une image, une méthode d’apprentissage automatique utilisera une très grande série d’images contenant le visage en question et obtiendra un modèle qui permettra au système de reconnaître le visage dans une nouvelle image jamais analysée.

Afin d’assurer ce type d’apprentissage automatique, les chercheurs ont découvert (années 1980) des méthodes pour entraîner des réseaux de neurones artificiels disposés en plusieurs couches. Toutes les connexions et les pondérations entre les neurones du réseau sont ajustables. L’apprentissage est justement une modification des pondérations.

En utilisant un algorithme particulier dit « *de rétropropagation de gradient* » un système avec apprentissage profond est entraînable pour calculer/affiner ses pondérations. Plus on dispose de données pour entraîner un système d’apprentissage profond, plus il deviendra précis et performant.

Ces dernières années, les chercheurs ont développé des algorithmes d’apprentissage efficaces dans l’identification des pondérations dans les réseaux de neurones afin d’augmenter la pertinence des modèles (e.g. réseaux de neurones avec rétropropation – Hinton, ; réseaux convolutifs – Yann Le CUN ; réseaux antagonistes génératifs – Bengio & Goodfellow).

Ainsi, les méthodes d'apprentissage profond ont été à l'origine d'incroyables percées dans la vision par ordinateur, la reconnaissance de la parole, la traduction automatique, le traitement du langage naturel, l’analyse d’images médicales, les voitures autonomes, les jeux et la robotique. Exemples : Nvidia Face Generator ; #AlphaZero ; DeepDream Generator (Google) ; GPT-2 (#OpenAI) ; Springer Nature – premier livre généré par la machine sur les « Lithium-Ion Batteries »…

L’explication on peut la trouver dans :

 1. la massification de données (croissance exponentielle) ;
 2. l’augmentation de la capacité de calcul ;
 3. le développement de nouvelles architectures algorithmiques.

> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTUxMTcyNTY4M119
-->